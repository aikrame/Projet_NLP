{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dictionary(dict):\n",
    "    \"\"\"\n",
    "    Extends python dictionary in order to have\n",
    "    index --> word\n",
    "    but also\n",
    "    word --> index\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(dictionary, self).__init__()\n",
    "        self.index = {}\n",
    "        self.size = 0\n",
    "    \n",
    "    def __setitem__(self, key, value):\n",
    "        super(dictionary, self).__setitem__(key, value)\n",
    "        self.index[value] = key\n",
    "        self.size += 1\n",
    "    \n",
    "    def __delitem__(self, key):\n",
    "        value = super().pop(key)\n",
    "        ignore = self.index.pop(value)\n",
    "        self.size -=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_corpus(corpus, context_size, dictionary, fixed_dictionary=False):\n",
    "    list_of_points = []\n",
    "    for document in corpus:\n",
    "        list_of_points += process_document(document, context_size, dictionary, fixed_dictionary)\n",
    "    return list_of_points\n",
    "\n",
    "\n",
    "def process_document(document, context_size, dictionary, fixed_dictionary=False):\n",
    "    text = document.lower()\n",
    "    p = re.compile(\"[a-z]+\")\n",
    "    tokens = p.findall(text)\n",
    "    list_of_points = []\n",
    "    for i in range(len(tokens) - context_size + 1):\n",
    "        data_point = [0 for l in range(context_size)]\n",
    "        add_new_data_point = True\n",
    "        for j in range(context_size):\n",
    "            k = i+j\n",
    "            if tokens[k] not in dictionary.index:\n",
    "                if fixed_dictionary:\n",
    "                    # only takes series of words in the dictionary\n",
    "                    add_new_data_point = False\n",
    "                    break\n",
    "                else:\n",
    "                    new_Ix = dictionary.size\n",
    "                    dictionary[new_Ix] = tokens[k]\n",
    "            data_point[j] = dictionary.index[tokens[k]]\n",
    "        if add_new_data_point:\n",
    "            list_of_points.append(tuple(data_point))\n",
    "    return list_of_points\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some important values\n",
    "CONTEXT_SIZE = 4\n",
    "DICT_SIZE = 17000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the arXiv corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>arxiv_primary_category</th>\n",
       "      <th>summary</th>\n",
       "      <th>published</th>\n",
       "      <td>updated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2001.05867v1</th>\n",
       "      <th>$Ïƒ$-Lacunary actions of Polish groups</th>\n",
       "      <th>Jan Grebik</th>\n",
       "      <th>math.LO</th>\n",
       "      <th>We show that every essentially countable orbit equivalence relation induced by a continuous action of a Polish group on a Polish space is $\\sigma$-lacunary. In combination with [Invent. Math.201 (1), 309-383, 2015] we obtain a straightforward proof of the result from [Adv. Math.307, 312-343,2017] that every essentially countable equivalence relation that is induced by an action of abelian non-archimedean Polish group is essentially hyperfinite.</th>\n",
       "      <th>2020-01-16T15:09:02Z</th>\n",
       "      <td>2020-01-16T15:09:02Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/1303.6933v1</th>\n",
       "      <th>Hans Grauert (1930-2011)</th>\n",
       "      <th>Alan Huckleberry</th>\n",
       "      <th>math.HO</th>\n",
       "      <th>Hans Grauert died in September of 2011. This article reviews his life in mathematics and recalls some detail his major accomplishments.</th>\n",
       "      <th>2013-03-27T19:23:57Z</th>\n",
       "      <td>2013-03-27T19:23:57Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/1407.3775v1</th>\n",
       "      <th>A New Proof of Stirling's Formula</th>\n",
       "      <th>Thorsten Neuschel</th>\n",
       "      <th>math.HO</th>\n",
       "      <th>A new simple proof of Stirling's formula via the partial fraction expansion for the tangent function is presented.</th>\n",
       "      <th>2014-07-10T11:26:39Z</th>\n",
       "      <td>2014-07-10T11:26:39Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/math/0307381v3</th>\n",
       "      <th>On Dequantization of Fedosov's Deformation Quantization</th>\n",
       "      <th>Alexander V. Karabegov</th>\n",
       "      <th>math.QA</th>\n",
       "      <th>To each natural deformation quantization on a Poisson manifold M we associate a Poisson morphism from the formal neighborhood of the zero section of the cotangent bundle to M to the formal neighborhood of the diagonal of the product M x M~, where M~ is a copy of M with the opposite Poisson structure. We call it dequantization of the natural deformation quantization. Then we \"dequantize\" Fedosov's quantization.</th>\n",
       "      <th>2003-07-30T06:20:33Z</th>\n",
       "      <td>2003-09-20T01:29:18Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/1902.05717v1</th>\n",
       "      <th>A New Smoothing Technique based on the Parallel Concatenation of   Forward/Backward Bayesian Filters: Turbo Smoothing</th>\n",
       "      <th>Giorgio M. Vitetta;Pasquale Di Viesti;Emilio Sirignano</th>\n",
       "      <th>stat.CO</th>\n",
       "      <th>Recently, a novel method for developing filtering algorithms, based on the parallel concatenation of Bayesian filters and called turbo filtering, has been proposed. In this manuscript we show how the same conceptual approach can be exploited to devise a new smoothing method, called turbo smoothing. A turbo smoother combines a turbo filter, employed in its forward pass, with the parallel concatenation of two backward information filters used in its backward pass. As a specific application of our general theory, a detailed derivation of two turbo smoothing algorithms for conditionally linear Gaussian systems is illustrated. Numerical results for a specific dynamic system evidence that these algorithms can achieve a better complexity-accuracy tradeoff than other smoothing techniques recently appeared in the literature.</th>\n",
       "      <th>2019-02-15T08:21:22Z</th>\n",
       "      <td>2019-02-15T08:21:22Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/1902.06861v2</th>\n",
       "      <th>Computation of the expected value of a function of a chi-distributed   random variable</th>\n",
       "      <th>Paul Kabaila;Nishika Ranathunga</th>\n",
       "      <th>stat.CO</th>\n",
       "      <th>We consider the problem of numerically evaluating the expected value of a smooth bounded function of a chi-distributed random variable, divided by the square root of the number of degrees of freedom. This problem arises in the contexts of simultaneous inference, the selection and ranking of populations and in the evaluation of multivariate t probabilities. It also arises in the assessment of the coverage probability and expected volume properties of the some non-standard confidence regions. We use a transformation put forward by Mori, followed by the application of the trapezoidal rule. This rule has the remarkable property that, for suitable integrands, it is exponentially convergent. We use it to create a nested sequence of quadrature rules, for the estimation of the approximation error, so that previous evaluations of the integrand are not wasted. The application of the trapezoidal rule requires the approximation of an infinite sum by a finite sum. We provide a new easily computed upper bound on the error of this approximation. Our overall conclusion is that this method is a very suitable candidate for the computation of the coverage and expected volume properties of non-standard confidence regions.</th>\n",
       "      <th>2019-02-19T02:23:36Z</th>\n",
       "      <td>2019-12-17T04:46:30Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/1902.07706v1</th>\n",
       "      <th>EcoMem: An R package for quantifying ecological memory</th>\n",
       "      <th>Malcolm S. Itter;Jarno Vanhatalo;Andrew O. Finley</th>\n",
       "      <th>stat.CO</th>\n",
       "      <th>Ecological processes may exhibit memory to past disturbances affecting the resilience of ecosystems to future disturbance. Understanding the role of ecological memory in shaping ecosystem responses to disturbance under global change is a critical step toward developing effective adaptive management strategies to maintain ecosystem function and biodiversity. We developed EcoMem, an R package for quantifying ecological memory functions using common environmental time series data (continuous, count, proportional) applying a Bayesian hierarchical framework. The package estimates memory functions for continuous and binary (e.g., disturbance chronology) variables making no a priori assumption on the form of the functions. EcoMem allows users to quantify ecological memory for a wide range of ecosystem processes and responses. The utility of the package to advance understanding of the memory of ecosystems to environmental drivers is demonstrated using a simulated dataset and a case study assessing the memory of boreal tree growth to insect defoliation.</th>\n",
       "      <th>2019-02-20T18:59:55Z</th>\n",
       "      <td>2019-02-20T18:59:55Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/1902.09029v1</th>\n",
       "      <th>Snowboot: Bootstrap Methods for Network Inference</th>\n",
       "      <th>Yuzhou Chen;Yulia R. Gel;Vyacheslav Lyubchich;Kusha Nezafati</th>\n",
       "      <th>stat.CO</th>\n",
       "      <th>Complex networks are used to describe a broad range of disparate social systems and natural phenomena, from power grids to customer segmentation to human brain connectome. Challenges of parametric model specification and validation inspire a search for more data-driven and flexible nonparametric approaches for inference of complex networks. In this paper we discuss methodology and R implementation of two bootstrap procedures on random networks, that is, patchwork bootstrap of Thompson et al. (2016) and Gel et al. (2017) and vertex bootstrap of Snijders and Borgatti (1999). To our knowledge, the new R package snowboot is the first implementation of the vertex and patchwork bootstrap inference on networks in R. Our new package is accompanied with a detailed user's manual, and is compatible with the popular R package on network studies igraph. We evaluate the patchwork bootstrap and vertex bootstrap with extensive simulation studies and illustrate their utility in application to analysis of real world networks.</th>\n",
       "      <th>2019-02-24T22:31:43Z</th>\n",
       "      <td>2019-02-24T22:31:43Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/1903.02699v2</th>\n",
       "      <th>Hamiltonian Monte Carlo on Symmetric and Homogeneous Spaces via   Symplectic Reduction</th>\n",
       "      <th>Alessandro Barp;Anthony Kennedy;Mark Girolami</th>\n",
       "      <th>stat.CO</th>\n",
       "      <th>The Hamiltonian Monte Carlo method generates samples by introducing a mechanical system that explores the target density. For distributions on manifolds it is not always simple to perform the mechanics as a result of the lack of global coordinates, the constraints of the manifold, and the requirement to compute the geodesic flow. In this paper we explain how to construct the Hamiltonian system on naturally reductive homogeneous spaces using symplectic reduction, which lifts the HMC scheme to a matrix Lie group with global coordinates and constant metric. This provides a general framework that is applicable to many manifolds that arise in applications, such as hyperspheres, hyperbolic spaces, symmetric positive-definite matrices, Grassmannian, and Stiefel manifolds.</th>\n",
       "      <th>2019-03-07T02:38:20Z</th>\n",
       "      <td>2019-04-19T00:35:59Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41063 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                          text\n",
       "id                                  title                                              authors                                            arxiv_primary_category summary                                            published                          updated\n",
       "http://arxiv.org/abs/2001.05867v1   $Ïƒ$-Lacunary actions of Polish groups              Jan Grebik                                         math.LO                We show that every essentially countable orbit ... 2020-01-16T15:09:02Z  2020-01-16T15:09:02Z\n",
       "http://arxiv.org/abs/1303.6933v1    Hans Grauert (1930-2011)                           Alan Huckleberry                                   math.HO                Hans Grauert died in September of 2011. This ar... 2013-03-27T19:23:57Z  2013-03-27T19:23:57Z\n",
       "http://arxiv.org/abs/1407.3775v1    A New Proof of Stirling's Formula                  Thorsten Neuschel                                  math.HO                A new simple proof of Stirling's formula via th... 2014-07-10T11:26:39Z  2014-07-10T11:26:39Z\n",
       "http://arxiv.org/abs/math/0307381v3 On Dequantization of Fedosov's Deformation Quan... Alexander V. Karabegov                             math.QA                To each natural deformation quantization on a P... 2003-07-30T06:20:33Z  2003-09-20T01:29:18Z\n",
       "...                                                                                                                                                                                                                                                        ...\n",
       "http://arxiv.org/abs/1902.05717v1   A New Smoothing Technique based on the Parallel... Giorgio M. Vitetta;Pasquale Di Viesti;Emilio Si... stat.CO                Recently, a novel method for developing filteri... 2019-02-15T08:21:22Z  2019-02-15T08:21:22Z\n",
       "http://arxiv.org/abs/1902.06861v2   Computation of the expected value of a function... Paul Kabaila;Nishika Ranathunga                    stat.CO                We consider the problem of numerically evaluati... 2019-02-19T02:23:36Z  2019-12-17T04:46:30Z\n",
       "http://arxiv.org/abs/1902.07706v1   EcoMem: An R package for quantifying ecological... Malcolm S. Itter;Jarno Vanhatalo;Andrew O. Finley  stat.CO                Ecological processes may exhibit memory to past... 2019-02-20T18:59:55Z  2019-02-20T18:59:55Z\n",
       "http://arxiv.org/abs/1902.09029v1   Snowboot: Bootstrap Methods for Network Inference  Yuzhou Chen;Yulia R. Gel;Vyacheslav Lyubchich;K... stat.CO                Complex networks are used to describe a broad r... 2019-02-24T22:31:43Z  2019-02-24T22:31:43Z\n",
       "http://arxiv.org/abs/1903.02699v2   Hamiltonian Monte Carlo on Symmetric and Homoge... Alessandro Barp;Anthony Kennedy;Mark Girolami      stat.CO                The Hamiltonian Monte Carlo method generates sa... 2019-03-07T02:38:20Z  2019-04-19T00:35:59Z\n",
       "\n",
       "[41063 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./arxiv_articles.csv\", sep=\"|\")\n",
    "s = time.time()\n",
    "mydict = dictionary()\n",
    "#dataset = process_corpus(data['summary'], CONTEXT_SIZE, mydict)\n",
    "#t = time.time() - s\n",
    "#print(\"Done in {} seconds\".format(int(t)))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64515"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5447489"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 2, 3),\n",
       " (1, 2, 3, 4),\n",
       " (2, 3, 4, 5),\n",
       " (3, 4, 5, 6),\n",
       " (4, 5, 6, 7),\n",
       " (5, 6, 7, 8),\n",
       " (6, 7, 8, 9),\n",
       " (7, 8, 9, 10),\n",
       " (8, 9, 10, 11),\n",
       " (9, 10, 11, 12)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3\n",
       "0  0  1  2  3\n",
       "1  1  2  3  4\n",
       "2  2  3  4  5\n",
       "3  3  4  5  6\n",
       "4  4  5  6  7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30       371927\n",
       "14       241963\n",
       "49       146379\n",
       "11       136600\n",
       "68       120327\n",
       "          ...  \n",
       "40063         1\n",
       "35965         1\n",
       "42447         1\n",
       "48251         1\n",
       "63425         1\n",
       "Name: 0, Length: 63202, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of occurrences for each word\n",
    "word_counts = data_df.iloc[:, 0].value_counts()\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the most frequent words: ('the', 'of', 'and', 'a')\n"
     ]
    }
   ],
   "source": [
    "print(\"These are the most frequent words: {}\".format((mydict[30], mydict[14], mydict[49], mydict[11])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we want to keep only a subset of all the words\n",
    "# we define a fixed size for the dictionary and we \n",
    "# keep the words, starting from the most frequent ones\n",
    "\n",
    "words2keep = word_counts.keys()[:DICT_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([   30,    14,    49,    11,    68,    22,     0,    19,    64,\n",
       "                2,\n",
       "            ...\n",
       "            55667, 37971, 22882, 18427, 41792, 25705,  6781, 14941, 25061,\n",
       "            45905],\n",
       "           dtype='int64', length=17000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words2keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we create a new dictionary with the\n",
    "# words selected in the previous step\n",
    "new_dictionary = dictionary()\n",
    "for i in range(len(words2keep)):\n",
    "    new_dictionary[i] = mydict[words2keep[i]]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 11 seconds\n"
     ]
    }
   ],
   "source": [
    "# With the new dictionary, build the new training dataset\n",
    "\n",
    "# Creating the training dataset using series of 4 words \n",
    "# appearing in the text\n",
    "s = time.time()\n",
    "new_dataset = process_corpus(data['summary'], CONTEXT_SIZE, new_dictionary, fixed_dictionary=True)\n",
    "t = time.time() - s\n",
    "print(\"Done in {} seconds\".format(int(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5056387"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dictionary.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 34, 9, 1229),\n",
       " (34, 9, 1229, 2101),\n",
       " (9, 1229, 2101, 9555),\n",
       " (1229, 2101, 9555, 1457),\n",
       " (2101, 9555, 1457, 2214),\n",
       " (9555, 1457, 2214, 790),\n",
       " (1457, 2214, 790, 757),\n",
       " (2214, 790, 757, 14),\n",
       " (790, 757, 14, 3),\n",
       " (757, 14, 3, 347)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We show that every essentially countable orbit equivalence relation induced by a continuous action of a Polish group on a Polish space is $\\\\sigma$-lacunary. In combination with [Invent. Math.201 (1), 309-383, 2015] we obtain a straightforward proof of the result from [Adv. Math.307, 312-343,2017] that every essentially countable equivalence relation that is induced by an action of abelian non-archimedean Polish group is essentially hyperfinite.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['summary'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we show that every\n"
     ]
    }
   ],
   "source": [
    "print(new_dictionary[6], new_dictionary[34], new_dictionary[9], new_dictionary[1229])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'the',\n",
       " 1: 'of',\n",
       " 2: 'and',\n",
       " 3: 'a',\n",
       " 4: 'to',\n",
       " 5: 'in',\n",
       " 6: 'we',\n",
       " 7: 'is',\n",
       " 8: 'for',\n",
       " 9: 'that',\n",
       " 10: 'with',\n",
       " 11: 'this',\n",
       " 12: 'on',\n",
       " 13: 'are',\n",
       " 14: 'by',\n",
       " 15: 'as',\n",
       " 16: 'an',\n",
       " 17: 'model',\n",
       " 18: 'be',\n",
       " 19: 'from',\n",
       " 20: 'which',\n",
       " 21: 'data',\n",
       " 22: 'can',\n",
       " 23: 'our',\n",
       " 24: 'it',\n",
       " 25: 'at',\n",
       " 26: 'time',\n",
       " 27: 'based',\n",
       " 28: 'these',\n",
       " 29: 'models',\n",
       " 30: 'or',\n",
       " 31: 'using',\n",
       " 32: 'have',\n",
       " 33: 'results',\n",
       " 34: 'show',\n",
       " 35: 'two',\n",
       " 36: 'method',\n",
       " 37: 'between',\n",
       " 38: 'paper',\n",
       " 39: 'also',\n",
       " 40: 'such',\n",
       " 41: 's',\n",
       " 42: 'has',\n",
       " 43: 'not',\n",
       " 44: 'new',\n",
       " 45: 'their',\n",
       " 46: 'study',\n",
       " 47: 'one',\n",
       " 48: 'analysis',\n",
       " 49: 'used',\n",
       " 50: 'approach',\n",
       " 51: 'high',\n",
       " 52: 'its',\n",
       " 53: 'distribution',\n",
       " 54: 'methods',\n",
       " 55: 'problem',\n",
       " 56: 'different',\n",
       " 57: 'present',\n",
       " 58: 'more',\n",
       " 59: 'both',\n",
       " 60: 'when',\n",
       " 61: 'been',\n",
       " 62: 'than',\n",
       " 63: 'well',\n",
       " 64: 'large',\n",
       " 65: 'number',\n",
       " 66: 'system',\n",
       " 67: 'non',\n",
       " 68: 'function',\n",
       " 69: 'use',\n",
       " 70: 'proposed',\n",
       " 71: 'algorithm',\n",
       " 72: 'but',\n",
       " 73: 'process',\n",
       " 74: 'some',\n",
       " 75: 'where',\n",
       " 76: 'information',\n",
       " 77: 'first',\n",
       " 78: 'all',\n",
       " 79: 'under',\n",
       " 80: 'market',\n",
       " 81: 'i',\n",
       " 82: 'how',\n",
       " 83: 'order',\n",
       " 84: 'other',\n",
       " 85: 'risk',\n",
       " 86: 'network',\n",
       " 87: 'into',\n",
       " 88: 'parameters',\n",
       " 89: 'propose',\n",
       " 90: 'may',\n",
       " 91: 'find',\n",
       " 92: 'provide',\n",
       " 93: 'over',\n",
       " 94: 'systems',\n",
       " 95: 'only',\n",
       " 96: 'properties',\n",
       " 97: 'structure',\n",
       " 98: 'rate',\n",
       " 99: 'e',\n",
       " 100: 'each',\n",
       " 101: 'optimal',\n",
       " 102: 'framework',\n",
       " 103: 'dynamics',\n",
       " 104: 'theory',\n",
       " 105: 'here',\n",
       " 106: 'most',\n",
       " 107: 'stochastic',\n",
       " 108: 'state',\n",
       " 109: 'many',\n",
       " 110: 'however',\n",
       " 111: 'set',\n",
       " 112: 'linear',\n",
       " 113: 'case',\n",
       " 114: 'observed',\n",
       " 115: 'through',\n",
       " 116: 'general',\n",
       " 117: 'was',\n",
       " 118: 'space',\n",
       " 119: 'energy',\n",
       " 120: 'cell',\n",
       " 121: 'statistical',\n",
       " 122: 'performance',\n",
       " 123: 'networks',\n",
       " 124: 'random',\n",
       " 125: 'work',\n",
       " 126: 'estimation',\n",
       " 127: 'several',\n",
       " 128: 'will',\n",
       " 129: 'while',\n",
       " 130: 'n',\n",
       " 131: 'price',\n",
       " 132: 'they',\n",
       " 133: 'then',\n",
       " 134: 'given',\n",
       " 135: 'dimensional',\n",
       " 136: 'effects',\n",
       " 137: 'x',\n",
       " 138: 'field',\n",
       " 139: 'numerical',\n",
       " 140: 'sample',\n",
       " 141: 'mass',\n",
       " 142: 'low',\n",
       " 143: 'level',\n",
       " 144: 'simple',\n",
       " 145: 'm',\n",
       " 146: 'if',\n",
       " 147: 'important',\n",
       " 148: 'particular',\n",
       " 149: 'known',\n",
       " 150: 'observations',\n",
       " 151: 'within',\n",
       " 152: 'processes',\n",
       " 153: 'value',\n",
       " 154: 'simulations',\n",
       " 155: 'financial',\n",
       " 156: 'there',\n",
       " 157: 'three',\n",
       " 158: 'consider',\n",
       " 159: 'parameter',\n",
       " 160: 'single',\n",
       " 161: 'density',\n",
       " 162: 'probability',\n",
       " 163: 'about',\n",
       " 164: 'small',\n",
       " 165: 'conditions',\n",
       " 166: 'demonstrate',\n",
       " 167: 'effect',\n",
       " 168: 'distributions',\n",
       " 169: 'real',\n",
       " 170: 'size',\n",
       " 171: 'type',\n",
       " 172: 'due',\n",
       " 173: 'studies',\n",
       " 174: 'simulation',\n",
       " 175: 'very',\n",
       " 176: 'functions',\n",
       " 177: 'problems',\n",
       " 178: 'learning',\n",
       " 179: 'test',\n",
       " 180: 'regression',\n",
       " 181: 'possible',\n",
       " 182: 'long',\n",
       " 183: 'power',\n",
       " 184: 'series',\n",
       " 185: 'bayesian',\n",
       " 186: 'obtained',\n",
       " 187: 'any',\n",
       " 188: 'inference',\n",
       " 189: 'scale',\n",
       " 190: 'evolution',\n",
       " 191: 'found',\n",
       " 192: 'd',\n",
       " 193: 'were',\n",
       " 194: 'p',\n",
       " 195: 'cells',\n",
       " 196: 'result',\n",
       " 197: 'stars',\n",
       " 198: 'recent',\n",
       " 199: 'algorithms',\n",
       " 200: 'point',\n",
       " 201: 'allows',\n",
       " 202: 'including',\n",
       " 203: 'form',\n",
       " 204: 'mean',\n",
       " 205: 'complex',\n",
       " 206: 'standard',\n",
       " 207: 'range',\n",
       " 208: 'class',\n",
       " 209: 't',\n",
       " 210: 'out',\n",
       " 211: 'g',\n",
       " 212: 'selection',\n",
       " 213: 'r',\n",
       " 214: 'multiple',\n",
       " 215: 'growth',\n",
       " 216: 'no',\n",
       " 217: 'discuss',\n",
       " 218: 'volatility',\n",
       " 219: 'shown',\n",
       " 220: 'measure',\n",
       " 221: 'provides',\n",
       " 222: 'applied',\n",
       " 223: 'terms',\n",
       " 224: 'local',\n",
       " 225: 'estimate',\n",
       " 226: 'population',\n",
       " 227: 'applications',\n",
       " 228: 'efficient',\n",
       " 229: 'available',\n",
       " 230: 'up',\n",
       " 231: 'potential',\n",
       " 232: 'variables',\n",
       " 233: 'protein',\n",
       " 234: 'computational',\n",
       " 235: 'matrix',\n",
       " 236: 'developed',\n",
       " 237: 'theoretical',\n",
       " 238: 'solution',\n",
       " 239: 'gene',\n",
       " 240: 'specific',\n",
       " 241: 'control',\n",
       " 242: 'k',\n",
       " 243: 'so',\n",
       " 244: 'novel',\n",
       " 245: 'same',\n",
       " 246: 'noise',\n",
       " 247: 'ray',\n",
       " 248: 'introduce',\n",
       " 249: 'features',\n",
       " 250: 'even',\n",
       " 251: 'values',\n",
       " 252: 'compared',\n",
       " 253: 'experiments',\n",
       " 254: 'develop',\n",
       " 255: 'various',\n",
       " 256: 'finite',\n",
       " 257: 'associated',\n",
       " 258: 'estimator',\n",
       " 259: 'rates',\n",
       " 260: 'empirical',\n",
       " 261: 'carlo',\n",
       " 262: 'monte',\n",
       " 263: 'likelihood',\n",
       " 264: 'behavior',\n",
       " 265: 'error',\n",
       " 266: 'often',\n",
       " 267: 'star',\n",
       " 268: 'application',\n",
       " 269: 'like',\n",
       " 270: 'b',\n",
       " 271: 'during',\n",
       " 272: 'investigate',\n",
       " 273: 'way',\n",
       " 274: 'expected',\n",
       " 275: 'second',\n",
       " 276: 'significant',\n",
       " 277: 'those',\n",
       " 278: 'derive',\n",
       " 279: 'correlation',\n",
       " 280: 'design',\n",
       " 281: 'main',\n",
       " 282: 'equation',\n",
       " 283: 'formation',\n",
       " 284: 'approaches',\n",
       " 285: 'related',\n",
       " 286: 'c',\n",
       " 287: 'dynamic',\n",
       " 288: 'us',\n",
       " 289: 'further',\n",
       " 290: 'thus',\n",
       " 291: 'techniques',\n",
       " 292: 'via',\n",
       " 293: 'spatial',\n",
       " 294: 'describe',\n",
       " 295: 'experimental',\n",
       " 296: 'free',\n",
       " 297: 'multi',\n",
       " 298: 'equations',\n",
       " 299: 'consistent',\n",
       " 300: 'interest',\n",
       " 301: 'gaussian',\n",
       " 302: 'finally',\n",
       " 303: 'current',\n",
       " 304: 'interactions',\n",
       " 305: 'existing',\n",
       " 306: 'without',\n",
       " 307: 'research',\n",
       " 308: 'phase',\n",
       " 309: 'times',\n",
       " 310: 'among',\n",
       " 311: 'asymptotic',\n",
       " 312: 'frequency',\n",
       " 313: 'presented',\n",
       " 314: 'modeling',\n",
       " 315: 'strategy',\n",
       " 316: 'individual',\n",
       " 317: 'could',\n",
       " 318: 'functional',\n",
       " 319: 'stellar',\n",
       " 320: 'underlying',\n",
       " 321: 'similar',\n",
       " 322: 'sequence',\n",
       " 323: 'factor',\n",
       " 324: 'impact',\n",
       " 325: 'approximation',\n",
       " 326: 'changes',\n",
       " 327: 'stock',\n",
       " 328: 'source',\n",
       " 329: 'accuracy',\n",
       " 330: 'h',\n",
       " 331: 'measures',\n",
       " 332: 'procedure',\n",
       " 333: 'markov',\n",
       " 334: 'maximum',\n",
       " 335: 'l',\n",
       " 336: 'portfolio',\n",
       " 337: 'sampling',\n",
       " 338: 'variable',\n",
       " 339: 'economic',\n",
       " 340: 'response',\n",
       " 341: 'average',\n",
       " 342: 'common',\n",
       " 343: 'light',\n",
       " 344: 'role',\n",
       " 345: 'estimates',\n",
       " 346: 'detection',\n",
       " 347: 'continuous',\n",
       " 348: 'them',\n",
       " 349: 'expression',\n",
       " 350: 'dna',\n",
       " 351: 'example',\n",
       " 352: 'apply',\n",
       " 353: 'mechanism',\n",
       " 354: 'short',\n",
       " 355: 'variance',\n",
       " 356: 'obtain',\n",
       " 357: 'higher',\n",
       " 358: 'signal',\n",
       " 359: 'limit',\n",
       " 360: 'prices',\n",
       " 361: 'derived',\n",
       " 362: 'structures',\n",
       " 363: 'emission',\n",
       " 364: 'biological',\n",
       " 365: 'estimators',\n",
       " 366: 'across',\n",
       " 367: 'statistics',\n",
       " 368: 'optimization',\n",
       " 369: 'after',\n",
       " 370: 'recently',\n",
       " 371: 'term',\n",
       " 372: 'called',\n",
       " 373: 'technique',\n",
       " 374: 'change',\n",
       " 375: 'understanding',\n",
       " 376: 'cases',\n",
       " 377: 'key',\n",
       " 378: 'considered',\n",
       " 379: 'robust',\n",
       " 380: 'literature',\n",
       " 381: 'sources',\n",
       " 382: 'better',\n",
       " 383: 'examples',\n",
       " 384: 'measurements',\n",
       " 385: 'dependent',\n",
       " 386: 'development',\n",
       " 387: 'global',\n",
       " 388: 'presence',\n",
       " 389: 'law',\n",
       " 390: 'sets',\n",
       " 391: 'do',\n",
       " 392: 'equilibrium',\n",
       " 393: 'strong',\n",
       " 394: 'states',\n",
       " 395: 'human',\n",
       " 396: 'pricing',\n",
       " 397: 'previous',\n",
       " 398: 'diffusion',\n",
       " 399: 'classical',\n",
       " 400: 'strategies',\n",
       " 401: 'solutions',\n",
       " 402: 'markets',\n",
       " 403: 'account',\n",
       " 404: 'magnetic',\n",
       " 405: 'wide',\n",
       " 406: 'prediction',\n",
       " 407: 'surface',\n",
       " 408: 'does',\n",
       " 409: 'compare',\n",
       " 410: 'factors',\n",
       " 411: 'regions',\n",
       " 412: 'context',\n",
       " 413: 'components',\n",
       " 414: 'prove',\n",
       " 415: 'molecular',\n",
       " 416: 'independent',\n",
       " 417: 'identify',\n",
       " 418: 'give',\n",
       " 419: 'self',\n",
       " 420: 'future',\n",
       " 421: 'physical',\n",
       " 422: 'disk',\n",
       " 423: 'analyze',\n",
       " 424: 'treatment',\n",
       " 425: 'studied',\n",
       " 426: 'interaction',\n",
       " 427: 'because',\n",
       " 428: 'gamma',\n",
       " 429: 'corresponding',\n",
       " 430: 'binary',\n",
       " 431: 'gas',\n",
       " 432: 'dependence',\n",
       " 433: 'much',\n",
       " 434: 'resulting',\n",
       " 435: 'illustrate',\n",
       " 436: 'period',\n",
       " 437: 'nonlinear',\n",
       " 438: 'differential',\n",
       " 439: 'cost',\n",
       " 440: 'since',\n",
       " 441: 'cross',\n",
       " 442: 'suggest',\n",
       " 443: 'spectral',\n",
       " 444: 'distance',\n",
       " 445: 'types',\n",
       " 446: 'trading',\n",
       " 447: 'addition',\n",
       " 448: 'years',\n",
       " 449: 'mathematical',\n",
       " 450: 'tests',\n",
       " 451: 'efficiency',\n",
       " 452: 'being',\n",
       " 453: 'black',\n",
       " 454: 'group',\n",
       " 455: 'genes',\n",
       " 456: 'able',\n",
       " 457: 'article',\n",
       " 458: 'ratio',\n",
       " 459: 'solar',\n",
       " 460: 'asset',\n",
       " 461: 'should',\n",
       " 462: 'species',\n",
       " 463: 'particle',\n",
       " 464: 'conditional',\n",
       " 465: 'agents',\n",
       " 466: 'discrete',\n",
       " 467: 'planets',\n",
       " 468: 'resolution',\n",
       " 469: 'planet',\n",
       " 470: 'evidence',\n",
       " 471: 'lower',\n",
       " 472: 'fast',\n",
       " 473: 'events',\n",
       " 474: 'highly',\n",
       " 475: 'relative',\n",
       " 476: 'patterns',\n",
       " 477: 'around',\n",
       " 478: 'driven',\n",
       " 479: 'shows',\n",
       " 480: 'loss',\n",
       " 481: 'best',\n",
       " 482: 'uncertainty',\n",
       " 483: 'fields',\n",
       " 484: 'significantly',\n",
       " 485: 'physics',\n",
       " 486: 'increase',\n",
       " 487: 'make',\n",
       " 488: 'cancer',\n",
       " 489: 'least',\n",
       " 490: 'quantum',\n",
       " 491: 'al',\n",
       " 492: 'complexity',\n",
       " 493: 'effective',\n",
       " 494: 'utility',\n",
       " 495: 'posterior',\n",
       " 496: 'index',\n",
       " 497: 'line',\n",
       " 498: 'transition',\n",
       " 499: 'flow',\n",
       " 500: 'component',\n",
       " 501: 'estimated',\n",
       " 502: 'motion',\n",
       " 503: 'therefore',\n",
       " 504: 'activity',\n",
       " 505: 'velocity',\n",
       " 506: 'introduced',\n",
       " 507: 'clustering',\n",
       " 508: 'proteins',\n",
       " 509: 'convergence',\n",
       " 510: 'natural',\n",
       " 511: 'step',\n",
       " 512: 'fundamental',\n",
       " 513: 'means',\n",
       " 514: 'leads',\n",
       " 515: 'methodology',\n",
       " 516: 'moreover',\n",
       " 517: 'clusters',\n",
       " 518: 'existence',\n",
       " 519: 'total',\n",
       " 520: 'allow',\n",
       " 521: 'certain',\n",
       " 522: 'few',\n",
       " 523: 'chain',\n",
       " 524: 'part',\n",
       " 525: 'constraints',\n",
       " 526: 'would',\n",
       " 527: 'initial',\n",
       " 528: 'positive',\n",
       " 529: 'temperature',\n",
       " 530: 'defined',\n",
       " 531: 'accurate',\n",
       " 532: 'samples',\n",
       " 533: 'determine',\n",
       " 534: 'code',\n",
       " 535: 'et',\n",
       " 536: 'alternative',\n",
       " 537: 'optical',\n",
       " 538: 'science',\n",
       " 539: 'target',\n",
       " 540: 'scheme',\n",
       " 541: 'assumptions',\n",
       " 542: 'generalized',\n",
       " 543: 'levels',\n",
       " 544: 'objects',\n",
       " 545: 'mechanisms',\n",
       " 546: 'knowledge',\n",
       " 547: 'genome',\n",
       " 548: 'prior',\n",
       " 549: 'genetic',\n",
       " 550: 'log',\n",
       " 551: 'good',\n",
       " 552: 'less',\n",
       " 553: 'world',\n",
       " 554: 'perform',\n",
       " 555: 'neural',\n",
       " 556: 'furthermore',\n",
       " 557: 'along',\n",
       " 558: 'full',\n",
       " 559: 'tool',\n",
       " 560: 'lead',\n",
       " 561: 'respect',\n",
       " 562: 'returns',\n",
       " 563: 'setting',\n",
       " 564: 'testing',\n",
       " 565: 'dust',\n",
       " 566: 'relevant',\n",
       " 567: 'described',\n",
       " 568: 'social',\n",
       " 569: 'normal',\n",
       " 570: 'representation',\n",
       " 571: 'multivariate',\n",
       " 572: 'simulated',\n",
       " 573: 'cluster',\n",
       " 574: 'survey',\n",
       " 575: 'either',\n",
       " 576: 'machine',\n",
       " 577: 'tools',\n",
       " 578: 'near',\n",
       " 579: 'o',\n",
       " 580: 'graph',\n",
       " 581: 'fixed',\n",
       " 582: 'region',\n",
       " 583: 'importance',\n",
       " 584: 'choice',\n",
       " 585: 'points',\n",
       " 586: 'sequences',\n",
       " 587: 'adaptive',\n",
       " 588: 'constant',\n",
       " 589: 'critical',\n",
       " 590: 'made',\n",
       " 591: 'nature',\n",
       " 592: 'review',\n",
       " 593: 'radio',\n",
       " 594: 'whose',\n",
       " 595: 'larger',\n",
       " 596: 'approximate',\n",
       " 597: 'spectra',\n",
       " 598: 'focus',\n",
       " 599: 'whether',\n",
       " 600: 'f',\n",
       " 601: 'useful',\n",
       " 602: 'provided',\n",
       " 603: 'ii',\n",
       " 604: 'search',\n",
       " 605: 'structural',\n",
       " 606: 'dynamical',\n",
       " 607: 'direct',\n",
       " 608: 'classification',\n",
       " 609: 'support',\n",
       " 610: 'comparison',\n",
       " 611: 'wave',\n",
       " 612: 'spectrum',\n",
       " 613: 'alpha',\n",
       " 614: 'stability',\n",
       " 615: 'central',\n",
       " 616: 'analytical',\n",
       " 617: 'co',\n",
       " 618: 'need',\n",
       " 619: 'zero',\n",
       " 620: 'scales',\n",
       " 621: 'sparse',\n",
       " 622: 'sensitivity',\n",
       " 623: 'construct',\n",
       " 624: 'implementation',\n",
       " 625: 'stable',\n",
       " 626: 'length',\n",
       " 627: 'measured',\n",
       " 628: 'computation',\n",
       " 629: 'options',\n",
       " 630: 'estimating',\n",
       " 631: 'exact',\n",
       " 632: 'agent',\n",
       " 633: 'hypothesis',\n",
       " 634: 'extended',\n",
       " 635: 'errors',\n",
       " 636: 'option',\n",
       " 637: 'major',\n",
       " 638: 'additional',\n",
       " 639: 'increasing',\n",
       " 640: 'covariance',\n",
       " 641: 'degree',\n",
       " 642: 'include',\n",
       " 643: 'although',\n",
       " 644: 'correlations',\n",
       " 645: 'signals',\n",
       " 646: 'exponential',\n",
       " 647: 'telescope',\n",
       " 648: 'fluctuations',\n",
       " 649: 'chemical',\n",
       " 650: 'environment',\n",
       " 651: 'quality',\n",
       " 652: 'limited',\n",
       " 653: 'input',\n",
       " 654: 'basis',\n",
       " 655: 'negative',\n",
       " 656: 'decision',\n",
       " 657: 'particles',\n",
       " 658: 'predictions',\n",
       " 659: 'memory',\n",
       " 660: 'explain',\n",
       " 661: 'feature',\n",
       " 662: 'software',\n",
       " 663: 'making',\n",
       " 664: 'unknown',\n",
       " 665: 'four',\n",
       " 666: 'assumption',\n",
       " 667: 'open',\n",
       " 668: 'what',\n",
       " 669: 'measurement',\n",
       " 670: 'close',\n",
       " 671: 'bias',\n",
       " 672: 'generated',\n",
       " 673: 'matter',\n",
       " 674: 'identified',\n",
       " 675: 'directly',\n",
       " 676: 'domain',\n",
       " 677: 'explore',\n",
       " 678: 'improve',\n",
       " 679: 'deep',\n",
       " 680: 'variation',\n",
       " 681: 'investment',\n",
       " 682: 'fit',\n",
       " 683: 'groups',\n",
       " 684: 'practical',\n",
       " 685: 'imaging',\n",
       " 686: 'characteristics',\n",
       " 687: 'life',\n",
       " 688: 'explicit',\n",
       " 689: 'galaxy',\n",
       " 690: 'previously',\n",
       " 691: 'required',\n",
       " 692: 'requires',\n",
       " 693: 'performed',\n",
       " 694: 'datasets',\n",
       " 695: 'magnitude',\n",
       " 696: 'report',\n",
       " 697: 'parametric',\n",
       " 698: 'contrast',\n",
       " 699: 'influence',\n",
       " 700: 'uses',\n",
       " 701: 'likely',\n",
       " 702: 'partial',\n",
       " 703: 'distributed',\n",
       " 704: 'rather',\n",
       " 705: 'core',\n",
       " 706: 'evolutionary',\n",
       " 707: 'galaxies',\n",
       " 708: 'depends',\n",
       " 709: 'exchange',\n",
       " 710: 'concept',\n",
       " 711: 'scaling',\n",
       " 712: 'relationship',\n",
       " 713: 'reduction',\n",
       " 714: 'temporal',\n",
       " 715: 'widely',\n",
       " 716: 'condition',\n",
       " 717: 'modelling',\n",
       " 718: 'discussed',\n",
       " 719: 'against',\n",
       " 720: 'variety',\n",
       " 721: 'compute',\n",
       " 722: 'latent',\n",
       " 723: 'early',\n",
       " 724: 'property',\n",
       " 725: 'flux',\n",
       " 726: 'active',\n",
       " 727: 'predict',\n",
       " 728: 'difference',\n",
       " 729: 'computing',\n",
       " 730: 'confidence',\n",
       " 731: 'bound',\n",
       " 732: 'still',\n",
       " 733: 'earth',\n",
       " 734: 'galactic',\n",
       " 735: 'end',\n",
       " 736: 'j',\n",
       " 737: 'convex',\n",
       " 738: 'address',\n",
       " 739: 'vector',\n",
       " 740: 'shape',\n",
       " 741: 'sim',\n",
       " 742: 'fully',\n",
       " 743: 'extend',\n",
       " 744: 'quantitative',\n",
       " 745: 'precision',\n",
       " 746: 'generation',\n",
       " 747: 'image',\n",
       " 748: 'game',\n",
       " 749: 'establish',\n",
       " 750: 'background',\n",
       " 751: 'force',\n",
       " 752: 'complete',\n",
       " 753: 'view',\n",
       " 754: 'yet',\n",
       " 755: 'special',\n",
       " 756: 'images',\n",
       " 757: 'induced',\n",
       " 758: 'mixture',\n",
       " 759: 'following',\n",
       " 760: 'increases',\n",
       " 761: 'entropy',\n",
       " 762: 'costs',\n",
       " 763: 'family',\n",
       " 764: 'scientific',\n",
       " 765: 'findings',\n",
       " 766: 'planetary',\n",
       " 767: 'basic',\n",
       " 768: 'transport',\n",
       " 769: 'unique',\n",
       " 770: 'binding',\n",
       " 771: 'version',\n",
       " 772: 'produce',\n",
       " 773: 'volume',\n",
       " 774: 'molecules',\n",
       " 775: 'policy',\n",
       " 776: 'cosmic',\n",
       " 777: 'regime',\n",
       " 778: 'become',\n",
       " 779: 'sub',\n",
       " 780: 'typically',\n",
       " 781: 'massive',\n",
       " 782: 'traditional',\n",
       " 783: 'populations',\n",
       " 784: 'especially',\n",
       " 785: 'determined',\n",
       " 786: 'dimension',\n",
       " 787: 'speed',\n",
       " 788: 'trade',\n",
       " 789: 'bounds',\n",
       " 790: 'relation',\n",
       " 791: 'wealth',\n",
       " 792: 'brain',\n",
       " 793: 'return',\n",
       " 794: 'must',\n",
       " 795: 'detect',\n",
       " 796: 'correlated',\n",
       " 797: 'solve',\n",
       " 798: 'reaction',\n",
       " 799: 'differences',\n",
       " 800: 'pattern',\n",
       " 801: 'package',\n",
       " 802: 'per',\n",
       " 803: 'leading',\n",
       " 804: 'disease',\n",
       " 805: 'combination',\n",
       " 806: 'electron',\n",
       " 807: 'question',\n",
       " 808: 'variability',\n",
       " 809: 'ability',\n",
       " 810: 'far',\n",
       " 811: 'popular',\n",
       " 812: 'original',\n",
       " 813: 'processing',\n",
       " 814: 'regulatory',\n",
       " 815: 'cellular',\n",
       " 816: 'cannot',\n",
       " 817: 'production',\n",
       " 818: 'true',\n",
       " 819: 'together',\n",
       " 820: 'coefficients',\n",
       " 821: 'require',\n",
       " 822: 'radiation',\n",
       " 823: 'demonstrated',\n",
       " 824: 'strongly',\n",
       " 825: 'dataset',\n",
       " 826: 'experiment',\n",
       " 827: 'observation',\n",
       " 828: 'understand',\n",
       " 829: 'investigated',\n",
       " 830: 'necessary',\n",
       " 831: 'rm',\n",
       " 832: 'nonparametric',\n",
       " 833: 'elements',\n",
       " 834: 'finding',\n",
       " 835: 'designed',\n",
       " 836: 'presents',\n",
       " 837: 'achieve',\n",
       " 838: 'varying',\n",
       " 839: 'v',\n",
       " 840: 'kernel',\n",
       " 841: 'probabilities',\n",
       " 842: 'probabilistic',\n",
       " 843: 'curves',\n",
       " 844: 'makes',\n",
       " 845: 'description',\n",
       " 846: 'suitable',\n",
       " 847: 'evaluate',\n",
       " 848: 'threshold',\n",
       " 849: 'particularly',\n",
       " 850: 'causal',\n",
       " 851: 'suggests',\n",
       " 852: 'area',\n",
       " 853: 'identification',\n",
       " 854: 'weak',\n",
       " 855: 'predictive',\n",
       " 856: 'joint',\n",
       " 857: 'calibration',\n",
       " 858: 'tree',\n",
       " 859: 'closed',\n",
       " 860: 'detected',\n",
       " 861: 'implemented',\n",
       " 862: 'event',\n",
       " 863: 'sufficient',\n",
       " 864: 'amount',\n",
       " 865: 'task',\n",
       " 866: 'principle',\n",
       " 867: 'characterize',\n",
       " 868: 'beta',\n",
       " 869: 'computationally',\n",
       " 870: 'might',\n",
       " 871: 'theorem',\n",
       " 872: 'assets',\n",
       " 873: 'detailed',\n",
       " 874: 'construction',\n",
       " 875: 'established',\n",
       " 876: 'boundary',\n",
       " 877: 'waves',\n",
       " 878: 'overall',\n",
       " 879: 'matrices',\n",
       " 880: 'forward',\n",
       " 881: 'default',\n",
       " 882: 'fact',\n",
       " 883: 'formula',\n",
       " 884: 'inverse',\n",
       " 885: 'analyses',\n",
       " 886: 'reduce',\n",
       " 887: 'reduced',\n",
       " 888: 'aim',\n",
       " 889: 'curve',\n",
       " 890: 'z',\n",
       " 891: 'procedures',\n",
       " 892: 'weighted',\n",
       " 893: 'feedback',\n",
       " 894: 'subject',\n",
       " 895: 'water',\n",
       " 896: 'gravitational',\n",
       " 897: 'dark',\n",
       " 898: 'hierarchical',\n",
       " 899: 'upper',\n",
       " 900: 'mcmc',\n",
       " 901: 'equivalent',\n",
       " 902: 'body',\n",
       " 903: 'specifically',\n",
       " 904: 'take',\n",
       " 905: 'gives',\n",
       " 906: 'transfer',\n",
       " 907: 'product',\n",
       " 908: 'stationary',\n",
       " 909: 'plasma',\n",
       " 910: 'synthetic',\n",
       " 911: 'above',\n",
       " 912: 'extreme',\n",
       " 913: 'behaviour',\n",
       " 914: 'concentration',\n",
       " 915: 'q',\n",
       " 916: 'gradient',\n",
       " 917: 'providing',\n",
       " 918: 'output',\n",
       " 919: 'orders',\n",
       " 920: 'arbitrary',\n",
       " 921: 'parallel',\n",
       " 922: 'disks',\n",
       " 923: 'simultaneously',\n",
       " 924: 'path',\n",
       " 925: 'combined',\n",
       " 926: 'computer',\n",
       " 927: 'generally',\n",
       " 928: 'goal',\n",
       " 929: 'instead',\n",
       " 930: 'according',\n",
       " 931: 'array',\n",
       " 932: 'intensity',\n",
       " 933: 'generate',\n",
       " 934: 'minimum',\n",
       " 935: 'age',\n",
       " 936: 'exhibit',\n",
       " 937: 'extension',\n",
       " 938: 'art',\n",
       " 939: 'fraction',\n",
       " 940: 'improved',\n",
       " 941: 'upon',\n",
       " 942: 'fitting',\n",
       " 943: 'countries',\n",
       " 944: 'classes',\n",
       " 945: 'objective',\n",
       " 946: 'illustrated',\n",
       " 947: 'position',\n",
       " 948: 'rules',\n",
       " 949: 'u',\n",
       " 950: 'allowing',\n",
       " 951: 'programming',\n",
       " 952: 'semi',\n",
       " 953: 'examine',\n",
       " 954: 'typical',\n",
       " 955: 'analyzed',\n",
       " 956: 'tissue',\n",
       " 957: 'implications',\n",
       " 958: 'towards',\n",
       " 959: 'lines',\n",
       " 960: 'help',\n",
       " 961: 'contribution',\n",
       " 962: 'agreement',\n",
       " 963: 'smaller',\n",
       " 964: 'radial',\n",
       " 965: 'stage',\n",
       " 966: 'modern',\n",
       " 967: 'indicate',\n",
       " 968: 'before',\n",
       " 969: 'rule',\n",
       " 970: 'calculations',\n",
       " 971: 'accretion',\n",
       " 972: 'appropriate',\n",
       " 973: 'capital',\n",
       " 974: 'evaluation',\n",
       " 975: 'primary',\n",
       " 976: 'marginal',\n",
       " 977: 'consistency',\n",
       " 978: 'program',\n",
       " 979: 'past',\n",
       " 980: 'criterion',\n",
       " 981: 'flexible',\n",
       " 982: 'depend',\n",
       " 983: 'another',\n",
       " 984: 'difficult',\n",
       " 985: 'community',\n",
       " 986: 'reference',\n",
       " 987: 'easily',\n",
       " 988: 'challenges',\n",
       " 989: 'thermal',\n",
       " 990: 'coupled',\n",
       " 991: 'discovery',\n",
       " 992: 'infrared',\n",
       " 993: 'sequential',\n",
       " 994: 'constructed',\n",
       " 995: 'describes',\n",
       " 996: 'hence',\n",
       " 997: 'european',\n",
       " 998: 'relatively',\n",
       " 999: 'enables',\n",
       " ...}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the ASRS corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_ASRS = pd.read_csv(\"./ASRS_data.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_ASRS['Narrative'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s = time.time()\n",
    "#mydict_ASRS = dictionary()\n",
    "#dataset_ASRS = process_corpus(data_ASRS['Narrative'], CONTEXT_SIZE, mydict_ASRS)\n",
    "#t = time.time() - s\n",
    "#print(\"Done in {} seconds\".format(int(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
