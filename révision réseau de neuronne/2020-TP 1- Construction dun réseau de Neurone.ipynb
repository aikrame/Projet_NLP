{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports sous Notebook Python\n",
    "Dans ce TP, vous allez essentiellement programmer des classes qui vous permettront de construire des réseaux de neurones. Ces classes seront enregistrées dans un fichier `Neural.py` qui contient déjà la classe `Generic`. Cependant, le comportement par défaut d'un Notebook quand on demande d'importer un fichier est de ne pas le relire !!! Ainsi vos modifications dans le fichier `Neural.py` ne seront pas prises en compte. Pour que ce soit le cas, il faut lancer les commandes suivantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import Neural as Neur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None)\n"
     ]
    }
   ],
   "source": [
    "import Neural as Neur\n",
    "L=Neur.Generic()\n",
    "print(L.backward(1))\n",
    "# Vous devez trouver\n",
    "# (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copiez-collez la classe `Generic` en une classe `Arctan` et essayez de voir si python recharge bien le fichier Neural quand vous lancez la commande `import`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'nb_entree' and 'nb_sortie'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a588c1f82fa7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNeural\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mNeur\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mL\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNeur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mArctan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'nb_entree' and 'nb_sortie'"
     ]
    }
   ],
   "source": [
    "import Neural as Neur\n",
    "L=Neur.Arctan()\n",
    "X=1\n",
    "print(L.backward(1))\n",
    "\n",
    "# Vous devez trouver\n",
    "# (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition d'une couche\n",
    "Le fichier `Neural.py` sera une librairie qui nous permettra de construire nos réseaux de neurones, nous allons au fur et à mesure construire nos différent types de couche.\n",
    "Toute nos couches auront par défaut au moins les mêmes fonctions et variables que la couche `Generic`, et elles peuvent en avoir plus. Mathématiquement, on rappelle que une couche est une fonction $F$ qui prend des données $X$ et qui rend un vecteur $Y$. La couche a en interne des `paramètres` qui ont vocation à êtres `appris`. Les paramètres sont notés $\\theta$. On a ainsi\n",
    "$$ Y=F(\\theta,X)$$\n",
    "\n",
    "Les fonctions sont :\n",
    "\n",
    "* `set_params` : sert à fixer $\\theta$, les paramètres internes de la couche.\n",
    "* `get_params` : sert à récupérer $\\theta$, les paramètres internes de la couche.\n",
    "* `forward` : Applique la fonction $F$ aux données $X$ et aux paramètres internes $\\theta$ et rend $Y$.\n",
    "* `backward` : Applique la rétro-propagation du gradient à un gradient de sortie `grad_sortie` et calcule \n",
    "  * le gradient local `grad_local` qui est le gradient par rapport aux paramètres internes\n",
    "  * le gradient d'entrée `grad_entree` qui est le gradient par rapport à la variable $X$\n",
    "\n",
    "Les variables internes sont :\n",
    "* `self.nb_params` : la taille du vecteur $\\theta$.\n",
    "\n",
    "On doit se souvenir d'un truc concernant les tailles de vecteur.\n",
    "* Le vecteur $X$ et `grad_entree` sont de même taille (`grad_entree` est le gradient par rapport à $X$).\n",
    "* Le vecteur $Y$ et `grad_sortie` sont de même taille (`grad_sortie` est le gradient par rapport à $Y$).\n",
    "* Le vecteur $\\theta$ et `grad_local` sont de même taille, qui est `self.nb_params` (`grad_local` est le gradient par rapport à $\\theta$).\n",
    "\n",
    "\n",
    "On remarque que les couches sauvent automatiquement la variable `X` qui leur est donnée dans la fonction `forward`. Cette variable est stockée dans `self.save_X`. Il est très utile de sauver la variable `X` car elle est réutilisée dans la fonction `backward`. Cependant, cela va rendre notre réseau de neurones très gourmand en mémoire. D'autres choix peuvent être faits, mais nous nous fixerons sur ces choix là pendant les TPs.\n",
    "\n",
    "# Structure des données\n",
    "\n",
    "Les données $X$, $Y$ et les paramètres $\\theta$ sont des tableaux n\n",
    "\n",
    "\n",
    "# Implémentation de la couche Arctan\n",
    "Passons maintenant à notre première couche, vous avez normalement copié-collé la classe `Generic` en une classe `Arctan`. Nous allons remplir cette classe.\n",
    "\n",
    "La couche `Arctan` est une couche qui prend comme vecteur d'entrée $X$ de taille $p$ et qui rend un vecteur $Y$ de taille $p$ tel que\n",
    "$$Y[i]=\\phi(X[i]) \\quad \\forall 1\\le i\\le p$$\n",
    "\n",
    "Où $\\phi$ est la fonction arctangente. Cette couche n'a pas de paramètres locaux (`self.nb_params=0`), les fonctions `set_params` et `get_params` sont vides . Le backward de cette couche est :\n",
    "$$ g_e[i]=\\phi'(X[i])g_s[i] \\quad \\forall 1\\le i\\le p$$\n",
    "où $g_e$ et $g_s$ sont respectivement le gradient d'entrée et le gradient de sortie. On rappelle que la variable `X` a été sauvegardée dans la variable `self.save_X`.\n",
    "\n",
    "Implémentez cette couche et testez le code suivant, pour le gradient local, ou pour `get_params` on rendra la valeur `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'nb_entree' and 'nb_sortie'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6027f8c0ba1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNeural\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mNeur\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mL\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNeur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mArctan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgrad_sortie\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'nb_entree' and 'nb_sortie'"
     ]
    }
   ],
   "source": [
    "import Neural as Neur\n",
    "L=Neur.Arctan()\n",
    "np.random.seed(10)\n",
    "X=np.random.randn(4,2)\n",
    "grad_sortie=np.random.randn(4,2)\n",
    "print('nb_params=',L.nb_params)\n",
    "print('forward=',L.forward(X))\n",
    "print('backward=',L.backward(grad_sortie))\n",
    "# Vous devez trouver\n",
    "#nb_params= 0\n",
    "#forward= [[ 0.92666583  0.62090688]\n",
    "# [-0.99647548 -0.00838365]\n",
    "# [ 0.55596017 -0.6240794 ]\n",
    "# [ 0.25952369  0.10812518]]\n",
    "#backward=  (None, array([[ 0.00154751, -0.11550505],\n",
    "#       [ 0.12780186,  1.20295282],\n",
    "#       [-0.69626624,  0.67715401],\n",
    "#       [ 0.21357394,  0.43995373]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D'autres couches\n",
    "Sur le modèle de la couche Arctan, on peut construire un certain nombre de couches en modifiant la fonction $\\Phi$ (et évidemment la fonction $\\Phi'$). Ce sont toutes des couches simples sans paramètres et qui ne font qu'appliquer une non-linéarité aux données. Voici un tableau de quelques couches utilisées, de leur $\\Phi$ et $\\Phi'$ correspondant :\n",
    "\n",
    "\n",
    "| Nom            |     $\\Phi(X)$        |        $\\Phi'(X)$                 |\n",
    "| :------------  | :---------------:    | ----------------------:           |\n",
    "| Sigmoïde       | $$\\frac 1 {1+e^{-X}}$$ | $$\\frac {e^{-X}} {(1+e^{-X})^2}$$ |\n",
    "| RELU           |   $$\\max(X,0)$$        |   $$\\max(\\frac{X}{|X|},0)$$          |\n",
    "| ABS            |     $$|X|$$            |        $$\\frac{X}{|X|}$$              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La structure des données\n",
    "Avant de continuer, il nous faut dire quelle est la structure des données.\n",
    "On suppose que l'on a $n$ données différentes dans $\\mathbb{R}^p$. Ces données sont stockées dans une grande matrice de taille $(p,n)$ dont la $j$-ème colonne est un vecteur de taille $\\mathbb{R}^p$ qui représente la $j$-eme donnée.\n",
    "On note cette matrice $X_j[i]$ avec $1\\le i \\le p$ et $1\\le j \\le n$ tel que le vecteur $X_j$ est la $j$-eme donnée d'entrée.\n",
    "Ainsi l'exemple suivant représente 4 données dans $\\mathbb{R}^3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "X =np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première opération que l'on veut faire est, étant donné une matrice $A$ de taille $(q,p)$, de trouver la matrice $Y$ de taille $(q,n)$ telle que pour chaque donnée $j$, on ait $Y_j=AX_j$. Dans l'exemple suivant $q=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A= [[1 2 3]\n",
      " [4 5 6]]\n",
      "Y= [[ 38  44  50  56]\n",
      " [ 83  98 113 128]]\n"
     ]
    }
   ],
   "source": [
    "A=np.array([[1,2,3],[4,5,6]])\n",
    "print('A=',A)\n",
    "Y=A.dot(X)\n",
    "print ('Y=',Y)\n",
    "# Vous devez trouver\n",
    "#A= [[1 2 3]\n",
    "# [4 5 6]]\n",
    "#Y= [[ 38  44  50  56]\n",
    "# [ 83  98 113 128]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant on veut ajouter à $Y$ un vecteur $b \\in \\mathbb{R}^q$ tel que pour chaque donnée $j$ on ait $Z_j=Y_j+b$. Pour cela on utilise la commande suivante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1.]\n",
      " [2. 2. 2. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "b=np.array([1,2])\n",
    "print(np.outer(b,np.ones(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Vous devez comprendre la commande précédente et vous en servir pour calculer le vecteur $Z$ tel que $Z_j=AX_j+b$ dans l'exemple suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 54.  62.  72.  81.]\n",
      " [ 86.  97. 116. 131.]]\n"
     ]
    }
   ],
   "source": [
    "X =np.array([[1,1,3,4],[5,6,7,8],[9,10,11,12]])\n",
    "A=np.array([[1,5,3],[4,5,6]])\n",
    "b=np.array([1,3])\n",
    "# Remplir ici!\n",
    "Z=A.dot(X)+np.outer(b,np.ones(4))\n",
    "print(Z)\n",
    "# Vous devez trouver\n",
    "#[[ 54.  62.  72.  81.]\n",
    "# [ 86.  97. 116. 131.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation de la couche Dense\n",
    "Une couche \"dense\" est une couche qui prend $X$ un vecteur de taille $p$ et rend $Y$ un vecteur de taille $q$ tel que\n",
    "$$Y=AX+b,$$\n",
    "Où $A$ est une matrice et $b$ un vecteur de taille $q$. La matrice $A$ et le vecteur $b$ sont des paramètres de la couche. \n",
    "Nous allons dans un premier temps s'intéresser uniquement à la fonction `__init__`.\n",
    "\n",
    "La fonction `__init__` prend en argument deux entiers `nb_entree` et `nb_sortie` (notés $p$ et $q$ ici) correspondant à respectivement à la taille des vecteurs d'entrée et la taille des vecteurs de sortie. Ces nombres doivent être stockés dans les variables internes `self.n_entree` et `self.n_sortie`. De plus la fonction `__init__` va tirer de manière aléatoire une matrice $A$ de taille $(q,p)$ (stockée dans `self.A`) et un vecteur $b$ (stocké dans `self.b`) de taille $q$. On utilisera un tirage selon une normale $(0,1)$ avec la fonction `random.randn` pour cela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "8\n",
      "A= [[ 1.3315865   0.71527897 -1.54540029]\n",
      " [-0.00838385  0.62133597 -0.72008556]]\n",
      "b= [0.26551159 0.10854853]\n"
     ]
    }
   ],
   "source": [
    "# TEST de la classe\n",
    "np.random.seed(10)\n",
    "import Neural as Neur\n",
    "L=Neur.Dense(3,2)\n",
    "print(L.n_entree)\n",
    "print(L.n_sortie)\n",
    "print(L.nb_params)\n",
    "print('A=',L.A)\n",
    "print('b=',L.b)\n",
    "# Vous devez trouver\n",
    "#3\n",
    "#2\n",
    "#8\n",
    "#A=[[ 1.3315865   0.71527897 -1.54540029]\n",
    "#[-0.00838385  0.62133597 -0.72008556]]\n",
    "#b=[ 0.26551159  0.10854853]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remplit maintenant la fonction `forward` de la classe `Layer`. Etant donné une matrice $X$ de taille $(p,n)$ (où $n$ représente le nombre de données), la fonction `forward` calcule $Y$ de taille $(q,n)$ telle que $Y_j=AX_j+b$ pour tout $j$ entre $1$ et $n$ (on rappelle que $n$ est donné par `X.shape[1]`). On utilisera la section précédente pour faire ce calcul, on fera notamment attention à ne pas utiliser le vecteur $b$ directement dans la somme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-8.73510967 -8.23364448 -7.73217929 -7.23071411]\n",
      " [-3.2739255  -3.38105894 -3.48819237 -3.59532581]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "import Neural as Neur\n",
    "L = Neur.Dense(3,2)\n",
    "X =np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])\n",
    "print(L.forward(X))\n",
    "# Vous devez trouver\n",
    "#[[-8.73510967 -8.23364448 -7.73217929 -7.23071411]\n",
    "# [-3.2739255  -3.38105894 -3.48819237 -3.59532581]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant nous intéresser aux fonctions `get_params` et `set_params`. La fonction `get_params` rend dans un grand vecteur les coefficients de la matrice $A$ suivis des coefficients du vecteur $b$. Pour récupérer les coefficients d'une matrice sous forme d'un grand tableau, il existe la fonction `ravel` sous python. La fonction `set_params` prend en argument un grand vecteur et remplit tout d'abord les coefficients de la matrice $A$ puis les coefficients du vecteur $b$ avec les coefficients du grand vecteur. Pour recréer, la matrice $A$, on utilisera la fonction `reshape` de numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params= [ 1.3315865   0.71527897 -1.54540029 -0.00838385  0.62133597 -0.72008556\n",
      "  0.26551159  0.10854853]\n",
      "theta= [ 0.00429143 -0.17460021  0.43302619  1.20303737 -0.96506567  1.02827408\n",
      "  0.22863013  0.44513761]\n",
      "A= [[ 0.00429143 -0.17460021  0.43302619]\n",
      " [ 1.20303737 -0.96506567  1.02827408]]\n",
      "b= [0.22863013 0.44513761]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "import Neural as Neur\n",
    "L = Neur.Dense(3,2)\n",
    "print('params=',L.get_params())\n",
    "theta=np.random.randn(8)\n",
    "L.set_params(theta)\n",
    "print('theta=',theta)\n",
    "print('A=',L.A)\n",
    "print('b=',L.b)\n",
    "#Vous devez trouver\n",
    "#params= [ 1.3315865   0.71527897 -1.54540029 -0.00838385  0.62133597 -0.72008556\n",
    "#  0.26551159  0.10854853]\n",
    "#theta= [ 0.00429143 -0.17460021  0.43302619  1.20303737 -0.96506567  1.02827408\n",
    "#  0.22863013  0.44513761]\n",
    "#A= [[ 0.00429143 -0.17460021  0.43302619]\n",
    "# [ 1.20303737 -0.96506567  1.02827408]]\n",
    "#b= [0.22863013 0.44513761]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " C'était simple... Maintenant il faut coder la rétropropagation pour la couche dense. On rappelle que si la couche est de taille $(p,q)$ avec $n$ données alors la fonction `backward` prend en entrée `grad_sortie` (noté $g_s$ ici) un vecteur de taille $(q,n)$, calcule le gradient par rapport à $A$ et $b$ (qui sont donc des vecteurs de taille $(q,p)$ et $q$ respectivement et qui sont notés $g_A$ et $g_b$) le transforme en un vecteur de taille `self.nb_params` (comme la fonction `get_params`) et rend un vecteur `grad_entree` de taille $(p,n)$ (noté $g_e$ )qui sert à rétropropager le gradient aux couches précédentes. Les formules de calcul sont :\n",
    "\n",
    "$$g_e = A^T g_s$$   \n",
    "$$g_A = g_s X^T$$\n",
    "$$g_b[i] = \\sum_j g_s[i,j]$$\n",
    "Implémentez la fonction backward de Layer_dense et testez votre code ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_local= [ 3.28032607  1.23844345 -0.16801192  1.43755815  1.28044748 -2.41352966\n",
      " -1.07006308  4.29345906]\n",
      "grad_entree= [[-2.64293715 -2.33547403  0.35346419  3.16406972]\n",
      " [-0.71643766 -0.2077372   0.25191937  2.57454243]\n",
      " [ 2.24722802  1.48977695 -0.48258083 -4.69240621]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "import Neural as Neur\n",
    "L = Neur.Dense(3,2)\n",
    "X=np.random.randn(3,4)\n",
    "grad_sortie=np.random.randn(2,4)\n",
    "L.forward(X)\n",
    "gl,ge=L.backward(grad_sortie)\n",
    "print('grad_local=',gl)\n",
    "print('grad_entree=',ge)\n",
    "#Vous devez trouver\n",
    "#grad_local= [ 3.28032607  1.23844345 -0.16801192  1.43755815  1.28044748 -2.41352966\n",
    "# -1.07006308  4.29345906]\n",
    "#grad_entree= [[-2.64293715 -2.33547403  0.35346419  3.16406972]\n",
    "# [-0.71643766 -0.2077372   0.25191937  2.57454243]\n",
    "# [ 2.24722802  1.48977695 -0.48258083 -4.69240621]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction de la couche de perte L2\n",
    "Nous allons maintenant construire une classe qui correspond à la couche de perte $L_2$. Cette couche n'a pas de paramètres, elle a des données $D$ stockées et le forward consiste à calculer \n",
    "$$Y=\\frac{1}{2}\\Vert X-D\\Vert^2.$$\n",
    "La variable $Y$ est un réel et c'est classiquement la dernière couche du réseau de Neurone, la couche qui nous permet de mesurer l'écart entre $X$ et les données $D$.\n",
    "\n",
    "Pour le backward, cette couche n'a pas besoin de gradient de sortie, elle n'a pas de gradient local et son gradient d'entrée est $X-D$. Implémentez la couche de perte $L_2$ dans une classe nommée `Loss_L2` et testez le code ci-dessous :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3.8338361400772234\n",
      "(None, array([[-1.06607492, -0.60673045],\n",
      "       [ 1.54969172, -0.16621636],\n",
      "       [-0.18830978,  1.92312293]]))\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "import Neural as Neur\n",
    "D=np.random.randn(3,2)\n",
    "X=np.random.randn(3,2)\n",
    "L = Neur.Loss_L2(D)\n",
    "print(L.nb_params)\n",
    "print(L.forward(X))\n",
    "print(L.backward(None))\n",
    "#Vous devez trouver\n",
    "#0\n",
    "#3.8338361400772234\n",
    "#(None, array([[-1.06607492, -0.60673045],\n",
    "#       [ 1.54969172, -0.16621636],\n",
    "#       [-0.18830978,  1.92312293]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction du réseau de Neurones\n",
    "Nous allons maintenant nous intéresser à la construction du réseau de Neurone en lui même. Un réseau de Neurone est essentiellement une liste de couches  que l'on exécute successivement.\n",
    "\n",
    "Pour des raisons pratiques, la classe Network a exactement les mêmes noms de variables que la classe Layer, en fait on pourrait (et c'est même quelquefois très intéressant) considérer les réseaux de neurones comme une couche et faire plusieurs couches de réseau de neurone. La classe d'un réseau de neurone se nommera `Network`\n",
    "\n",
    "On s'intéresse d'abord aux fonctions `__init__` et aux fonctions `set_params` et `get_params`. La fonction `__init__` prend en argument la variable `list_layers` qui est une liste de couches. Cette liste doit être sauvée dans une variable `self.list_layers`.\n",
    "\n",
    "La variable `self.nb_params` doit être calculée. Par définition, nous supposerons que `self.nb_params` est juste la somme des tailles du vecteur des paramètres de chaque couche. Il faut donc parcourir la liste des couches pour calculer cette somme.\n",
    "\n",
    "Ensuite il faut remplir les fonctions `set_params` et `get_params`, la fonction `set_params` prend en argument un vecteur de taille `self.nb_params` et affecte les premiers coefficients du vecteur à la première couche, puis les suivants à la deuxième couche, etc.. La fonction `get_params` lance successivement les fonctions `get_params` des couches de la liste `self.list_layers` et stocke tous les résultats dans un grand tableau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "params= [ 0.31935642  0.4609029  -0.21578989  0.98907246  0.31475378  2.46765106\n",
      " -1.50832149  0.62060066 -1.04513254 -0.79800882  1.98508459  1.74481415\n",
      " -1.85618548 -0.2227737  -0.06584785 -2.13171211 -0.04883051  0.39334122\n",
      "  0.21726515 -1.99439377  1.10770823  0.24454398 -0.06191203 -0.75389296\n",
      "  0.71195902  0.91826915 -0.48209314  0.08958761  0.82699862 -1.95451212\n",
      "  0.11747566 -1.90745689 -0.92290926  0.46975143 -0.14436676 -0.40013835\n",
      " -0.29598385  0.84820861  0.70683045 -0.78726893  0.29294072 -0.47080725\n",
      "  2.40432561 -0.73935674 -0.31282876 -0.34888192 -0.43902624  0.14110417\n",
      "  0.27304932 -1.61857075 -0.57311336 -1.32044755  1.23620533  2.46532508]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "import Neural as Neur\n",
    "D=np.random.randn(4,10)\n",
    "X=np.random.randn(3,10)\n",
    "\n",
    "L1=Neur.Dense(3,2)\n",
    "L2=Neur.Arctan()\n",
    "L3=Neur.Dense(2,6)\n",
    "L4=Neur.Arctan()\n",
    "L5=Neur.Dense(6,4)\n",
    "L6=Neur.Loss_L2(D)\n",
    "\n",
    "\n",
    "N=Neur.Network([L1,L2,L3,L4,L5,L6])\n",
    "           \n",
    "print(N.nb_params)\n",
    "print('params=',N.get_params())\n",
    "theta=np.random.randn(N.nb_params)\n",
    "N.set_params(theta)\n",
    "print(np.linalg.norm(N.get_params()-theta))\n",
    "# Vous devez obtenir\n",
    "#54\n",
    "#params= [ 0.31935642  0.4609029  -0.21578989  0.98907246  0.31475378  2.46765106\n",
    "# -1.50832149  0.62060066 -1.04513254 -0.79800882  1.98508459  1.74481415\n",
    "# -1.85618548 -0.2227737  -0.06584785 -2.13171211 -0.04883051  0.39334122\n",
    "#  0.21726515 -1.99439377  1.10770823  0.24454398 -0.06191203 -0.75389296\n",
    "#  0.71195902  0.91826915 -0.48209314  0.08958761  0.82699862 -1.95451212\n",
    "#  0.11747566 -1.90745689 -0.92290926  0.46975143 -0.14436676 -0.40013835\n",
    "# -0.29598385  0.84820861  0.70683045 -0.78726893  0.29294072 -0.47080725\n",
    "#  2.40432561 -0.73935674 -0.31282876 -0.34888192 -0.43902624  0.14110417\n",
    "#  0.27304932 -1.61857075 -0.57311336 -1.32044755  1.23620533  2.46532508]\n",
    "#0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va programmer le `forward` et le `backward` du réseau de Neurone. \n",
    "Pour le `forward`, il s'agit juste de prendre la variable `X`, de la copier dans une variable `Z` et de faire passer cette variable `Z` dans chaque couche de la liste. On rend le résultat final. Il est important de copier la variable `X` dans une variable temporaire, sinon notre variable `X` sera modifiée par l'algorithme !!\n",
    "Pour le backward, il faut juste faire passer le gradient dans la liste à l'envers. Pour parcourir la liste à l'envers, il faut utiliser la fonction `reverse` de python. Tous les gradients locaux doivent être sauvés dans un grand vecteur, en utilisant un code similaire à la fonction `get_params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= 221.22208566711836\n",
      "b= [ -0.98368166 -13.25213968  -6.43358648  21.26910607 -14.87907661\n",
      "  -0.35630809 -23.31164944  27.82326981  -8.54800412   0.60620022\n",
      "  19.75002508 -12.54967804  -2.11398405   7.88699083 -10.23572316\n",
      " -16.12049842 -55.35679888   5.89818847  35.3167564  -23.76380918\n",
      "   9.58230795 -20.60100053   2.13470619  12.16682271  60.20318534\n",
      " -37.40341197 -11.30231307  24.605031    -7.02968989 -29.58355645\n",
      "   3.70228854 -32.28218831 -30.89848629  24.74850639 -28.1221249\n",
      "   2.2068016  -15.89016799 -14.56113369  39.54725178 -21.85809805\n",
      "  37.99499607 -23.31829384  26.61064478   1.60179509  13.52040638\n",
      "  -4.05726552  13.42191519 -10.62904716  10.35644233  -3.94820181\n",
      "  -5.71322575 -27.67476451  38.3148709   13.97693659]\n",
      "c= [[ 9.62116626e+00 -4.87640076e+00  1.06443747e-01 -3.06919940e+00\n",
      "  -3.59066812e+00 -5.20284380e+00 -1.51608936e+00 -7.42926556e-01\n",
      "   1.58659223e+01  1.34791006e+01]\n",
      " [ 3.21815414e-01 -1.85582037e+00 -8.85800384e-01 -9.69444862e-01\n",
      "  -1.57117837e+00 -1.78510096e+00 -2.74246971e+00  1.78176598e-02\n",
      "   3.64128666e+00  3.84196736e+00]\n",
      " [ 3.17260799e+01 -1.13094336e+01  2.85752584e+00 -7.67787992e+00\n",
      "  -7.75070537e+00 -1.26159745e+01  2.58695138e+00 -2.57007417e+00\n",
      "   4.35516287e+01  3.48904212e+01]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "import Neural as Neur\n",
    "D=np.random.randn(4,10)\n",
    "X=np.random.randn(3,10)\n",
    "\n",
    "L1=Neur.Dense(3,2)\n",
    "L2=Neur.Arctan()\n",
    "L3=Neur.Dense(2,6)\n",
    "L4=Neur.Arctan()\n",
    "L5=Neur.Dense(6,4)\n",
    "L6=Neur.Loss_L2(D)\n",
    "\n",
    "\n",
    "N=Neur.Network([L1,L2,L3,L4,L5,L6])\n",
    "\n",
    "a=N.forward(X)\n",
    "b,c=N.backward(None)\n",
    "print('a=',a)\n",
    "print('b=',b)\n",
    "print('c=',c)\n",
    "# Vous devez trouver\n",
    "#a= 221.22208566711836\n",
    "#b= [ -0.98368166 -13.25213968  -6.43358648  21.26910607 -14.87907661\n",
    "#  -0.35630809 -23.31164944  27.82326981  -8.54800412   0.60620022\n",
    "#  19.75002508 -12.54967804  -2.11398405   7.88699083 -10.23572316\n",
    "# -16.12049842 -55.35679888   5.89818847  35.3167564  -23.76380918\n",
    "#   9.58230795 -20.60100053   2.13470619  12.16682271  60.20318534\n",
    "# -37.40341197 -11.30231307  24.605031    -7.02968989 -29.58355645\n",
    "#   3.70228854 -32.28218831 -30.89848629  24.74850639 -28.1221249\n",
    "#   2.2068016  -15.89016799 -14.56113369  39.54725178 -21.85809805\n",
    "#  37.99499607 -23.31829384  26.61064478   1.60179509  13.52040638\n",
    "#  -4.05726552  13.42191519 -10.62904716  10.35644233  -3.94820181\n",
    "#  -5.71322575 -27.67476451  38.3148709   13.97693659]\n",
    "#c= [[ 9.62116626e+00 -4.87640076e+00  1.06443747e-01 -3.06919940e+00\n",
    "#  -3.59066812e+00 -5.20284380e+00 -1.51608936e+00 -7.42926556e-01\n",
    "#   1.58659223e+01  1.34791006e+01]\n",
    "# [ 3.21815414e-01 -1.85582037e+00 -8.85800384e-01 -9.69444862e-01\n",
    "#  -1.57117837e+00 -1.78510096e+00 -2.74246971e+00  1.78176598e-02\n",
    "#   3.64128666e+00  3.84196736e+00]\n",
    "# [ 3.17260799e+01 -1.13094336e+01  2.85752584e+00 -7.67787992e+00\n",
    "#  -7.75070537e+00 -1.26159745e+01  2.58695138e+00 -2.57007417e+00\n",
    "#   4.35516287e+01  3.48904212e+01]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voilà c'est fini... Vous êtes fin prêts à faire des réseaux de neurone maintenant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
