{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application à la théorie de l'approximation\n",
    "Dans cette première partie, on a une fonction $f:\\mathbb R^n \\rightarrow \\mathbb R^q$ qui n'est connue que sur un certain nombre de points $n_{data}$. L'objectif est d'approximer cette fonction $f$ en dehors des points qui sont connus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import Neural as Neur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Première application \n",
    "Ici $f$ est une fonction de $[0,1]$ dans $\\mathbb{R}$ donnée par \n",
    "$x\\mapsto \\sin(6\\pi |x|^{3/2}) +x^2$. Les données d'entrées $x_i$ sont les 256 points répartis uniforméments sur $[0,1]$ et les données de sorties sont les 256 réels donnés par $y_i=f(x_i)$. créez ces données dans un tableau `x` et `y` respectivement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.linspace(0,1,256)\n",
    "y=np.zeros(256)\n",
    "for i in range(len(x)):\n",
    "    y[i]=np.sin(6*np.pi*(np.abs(x[i]))**(3/2))+x[i]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction d'activation que nous allons utiliser est la fonction\n",
    "$$x\\mapsto \\frac{1}{1+e^{-x}}$$\n",
    "Implémentez cette fonction d'activation ainsi que sa dérivée dans la classe `Sigmoid`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5        0.95257413 0.88079708]\n",
      "(None, array([0.        , 0.13552998, 0.20998717]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "L=Neur.Sigmoid()\n",
    "x2=np.array([0,3,2])\n",
    "print(L.forward(x2))\n",
    "print(L.backward(x2))\n",
    "# Vous devez trouver\n",
    "# [0.5        0.95257413 0.88079708]\n",
    "# (None, array([0.        , 0.13552998, 0.20998717]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximation par réseau de neurones profond\n",
    "Nous allons construire 2 réseaux de neurones, le premier, noté `N` sera une couche Dense de taille $(1,12)$ suivi d'une sigmoïde et d'une couche dense $(12,1)$. Ce réseau de Neurone sera l'approximation de notre fonction $f$. Nous allons aussi créer un réseau de neurone noté `N_a` qui sera `N` suivi d'une couche de perte en norme $L^2$. Nous nous servirons de `N_a` pour l'optimisation. Créez ces deux réseaux de neurones et utilisez `N_a` pour lancer un algorithme de gradient à pas fixe avec $2000$ itérations, en faisant attention à bien régler le pas (vous verrez c'est quasiment impossible). Votre algorithme doit sortir l'évolution de la fonction objectif le long des itérations. Vous afficherez aussi le plit des vraies données et des prédictions du réseau de Neurone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-9fcd593e2c8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNeur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mNeur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNeur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNeur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mN_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNeur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNeur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoss_L2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Notebooks\\Degournay\\Neural.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, list_layers)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m              \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_params\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mlist_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_params\u001b[0m\u001b[1;31m# Nombre de parametres de la couche\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_X\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist_layers\u001b[0m\u001b[1;31m# Parametre de sauvegarde des donnees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "np.random.seed(13)\n",
    "N=Neur.Network([Neur.Dense(1,12),Neur.Sigmoid(),Neur.Dense(12,1)])\n",
    "N_a=Neur.Network([N,Neur.Loss_L2(y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalement vous ne devez pas avoir réussi à trouver le bon pas. Nous allons donc lancer un algorithme d'optimisation de `scipy` qui s'appelle `BFGS`. Si vous créez une fonction `func(u)` qui vous calcule le coût et une fonction `nablafunc(u)` qui vous rend le gradient de la fonction `func`, alors l'algorithme de `BFGS` que nous allons utiliser se lance avec :\n",
    "`from scipy.optimize import minimize`\n",
    "`res=minimize(func, u, method='BFGS', jac=nablafunc, options={'gtol': 1e-6, 'disp': True, 'maxiter': 2000})`\n",
    "Dans le résultat `res`, il y a beaucoup d'information, mais le minimiseur est dans `res.x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-45bc04b31cc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNeur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNeur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNeur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNeur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mN_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNeur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNeur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoss_L2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(13)\n",
    "N=Neur.Network([Neur.Dense(1,12),Neur.Sigmoid(),Neur.Dense(12,1)])\n",
    "N_a=Neur.Network([N,Neur.Loss_L2(y)])\n",
    "\n",
    "\n",
    "def func(u):\n",
    "    return None\n",
    "\n",
    "def nablafunc(u):\n",
    "    return None\n",
    "\n",
    "u=N_a.get_params()\n",
    "np.random.seed(42)\n",
    "eps=1.e-4\n",
    "c=func(u)\n",
    "grad=nablafunc(u)\n",
    "for i in range(4) :\n",
    "    d=np.random.randn(u.shape[0])\n",
    "    c2=func(u+eps*d)\n",
    "    print((c2-c)/eps,np.dot(d,grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
